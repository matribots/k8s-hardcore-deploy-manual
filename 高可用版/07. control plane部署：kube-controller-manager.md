为了方便参考，文档中的一些命令、配置文件中加了注释。在实际部署过程中，需要将这些注释删除，否则运行时会有问题。

&nbsp;

# 1. 前置步骤

- 完成 01.前置准备
- 完成 02. 创建私有CA（单root CA）
- 完成 03. 配置loadbalancer
- 完成 04. 为kubectl配置admin user
- 完成 05. control plane部署：etcd集群部署
- 完成 06. control plane部署：kube-apiserver

&nbsp;

# 2.2. 部署kube-controller-manager

## 2.1. 创建kube-controller-manager证书签名请求

```bash
# ssh to operation-machine
$ ssh kube@192.168.1.200

$ mkdir -p /opt/kubernetes/pki/kube-controller-manager/cert

$ vim /opt/kubernetes/pki/kube-controller-manager/kube-controller-manager-csr.json
```

```bash
{
  "CN": "system:kube-controller-manager",
  "hosts": [],
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "Shanghai",
      "L": "Shanghai",
      "O": "system:kube-controller-manager",
      "OU": "System"
    }
  ]
}
```

- 相较于`kube-apiserver`，`kube-controller-manager`为其客户端，为客户端签发证书可以不指定`hosts`。

- 证书请求中，`CN`字段代表`kube-controller-manager`在k8s系统中的用户， `O`字段的值代表`kube-controller-manager`在k8s系统中的 group为`system:kube-controller-manager`；而`RBAC`预定义的 ClusterRoleBinding已将用户`system:kube-controller-manager` 与 ClusterRole `system:kube-controller-manager`绑定，这就赋予了kube-controller-manager在集群中对应的权限。

## 2.2. 生成kube-controller-manager证书和私钥

```bash
# ssh to operation-machine
$ ssh kube@192.168.1.200

$ cfssl gencert \
-ca=/opt/kubernetes/pki/ca/cert/ca.pem \
-ca-key=/opt/kubernetes/pki/ca/cert/ca-key.pem \
-config=/opt/kubernetes/pki/ca/ca-config.json \
-profile=kubernetes /opt/kubernetes/pki/kube-controller-manager/kube-controller-manager-csr.json | cfssljson -bare /opt/kubernetes/pki/kube-controller-manager/cert/kube-controller-manager

$ ls /opt/kubernetes/pki/kube-controller-manager/cert/
... kube-controller-manager-key.pem kube-controller-manager.pem ...
```

## 2.3. 分发kube-controller-manager的证书和私钥

```bash
# ssh to operation-machine
$ ssh kube@192.168.1.200

$ ssh kube@192.168.1.204 "mkdir -p /opt/kubernetes/pki/kube-controller-manager/cert/" && \
  ssh kube@192.168.1.205 "mkdir -p /opt/kubernetes/pki/kube-controller-manager/cert/" && \
  ssh kube@192.168.1.206 "mkdir -p /opt/kubernetes/pki/kube-controller-manager/cert/"

$ scp /opt/kubernetes/pki/kube-controller-manager/cert/kube-controller-manager*.pem kube@192.168.1.204:/opt/kubernetes/pki/kube-controller-manager/cert/ && \
  scp /opt/kubernetes/pki/kube-controller-manager/cert/kube-controller-manager*.pem kube@192.168.1.205:/opt/kubernetes/pki/kube-controller-manager/cert/ && \
  scp /opt/kubernetes/pki/kube-controller-manager/cert/kube-controller-manager*.pem kube@192.168.1.206:/opt/kubernetes/pki/kube-controller-manager/cert/
```

## 2.4. 生成kube-controller-manager.kubeconfig

kubeconfig 文件组织有关集群、用户、命名空间和身份认证机制的信息。 kube-apiserver客户端通常使用 kubeconfig 文件来查找选择集群所需的信息，并与集群的 API 服务器进行通信。相对于kube-apiserver，kube-controller-manager、kube-scheduler、kubelet、kubectl都是其客户端。

```bash
# ssh to operation-machine
$ ssh kube@192.168.1.200

$ kubectl config set-cluster kubernetes --certificate-authority=/opt/kubernetes/pki/ca/cert/ca.pem --embed-certs=true --server=https://192.168.1.210:6443 --kubeconfig=/opt/kubernetes/kubeconfig/kube-controller-manager.kubeconfig

$ kubectl config set-credentials kube-controller-manager --client-certificate=/opt/kubernetes/pki/kube-controller-manager/cert/kube-controller-manager.pem --embed-certs=true --client-key=/opt/kubernetes/pki/kube-controller-manager/cert/kube-controller-manager-key.pem --kubeconfig=/opt/kubernetes/kubeconfig/kube-controller-manager.kubeconfig

# 注：--user参数的值和CSR中CN字段的值一致
$ kubectl config set-context default --cluster=kubernetes --user=kube-controller-manager --kubeconfig=/opt/kubernetes/kubeconfig/kube-controller-manager.kubeconfig

$ kubectl config use-context default --kubeconfig=/opt/kubernetes/kubeconfig/kube-controller-manager.kubeconfig
```

## 2.5. 分发kube-controller-manager.kubeconfig

```bash
# ssh operation-machine
$ ssh kube@192.168.1.200

$ scp /opt/kubernetes/kubeconfig/kube-controller-manager.kubeconfig kube@192.168.1.204:/opt/kubernetes/kubeconfig/kube-controller-manager.kubeconfig && \
  scp /opt/kubernetes/kubeconfig/kube-controller-manager.kubeconfig kube@192.168.1.205:/opt/kubernetes/kubeconfig/kube-controller-manager.kubeconfig && \
  scp /opt/kubernetes/kubeconfig/kube-controller-manager.kubeconfig kube@192.168.1.206:/opt/kubernetes/kubeconfig/kube-controller-manager.kubeconfig
```

## 2.6. 启动kube-controller-manager

```bash
# ssh k8s-master-01
$ ssh kube@192.168.1.204

$ /opt/kubernetes/bin/kube-controller-manager \
  --allocate-node-cidrs=true \
  --authentication-kubeconfig=/opt/kubernetes/kubeconfig/kube-controller-manager.kubeconfig \
  --authorization-kubeconfig=/opt/kubernetes/kubeconfig/kube-controller-manager.kubeconfig \
  --bind-address=192.168.1.204 \
  --cluster-cidr=172.20.0.0/16 \
  --cluster-name=kubernetes \
  --cluster-signing-cert-file=/opt/kubernetes/pki/ca/cert/ca.pem \
  --cluster-signing-key-file=/opt/kubernetes/pki/ca/cert/ca-key.pem \
  --kubeconfig=/opt/kubernetes/kubeconfig/kube-controller-manager.kubeconfig \
  --leader-elect=true \
  --node-cidr-mask-size=24 \
  --root-ca-file=/opt/kubernetes/pki/ca/cert/ca.pem \
  --service-account-private-key-file=/opt/kubernetes/pki/ca/cert/ca-key.pem \
  --service-cluster-ip-range=10.68.0.0/16 \
  --use-service-account-credentials=true \
  --v=2
```

```bash
# ssh k8s-master-02
$ ssh kube@192.168.1.205

$ /opt/kubernetes/bin/kube-controller-manager \
  --allocate-node-cidrs=true \
  --authentication-kubeconfig=/opt/kubernetes/kubeconfig/kube-controller-manager.kubeconfig \
  --authorization-kubeconfig=/opt/kubernetes/kubeconfig/kube-controller-manager.kubeconfig \
  --bind-address=192.168.1.205 \
  --cluster-cidr=172.20.0.0/16 \
  --cluster-name=kubernetes \
  --cluster-signing-cert-file=/opt/kubernetes/pki/ca/cert/ca.pem \
  --cluster-signing-key-file=/opt/kubernetes/pki/ca/cert/ca-key.pem \
  --kubeconfig=/opt/kubernetes/kubeconfig/kube-controller-manager.kubeconfig \
  --leader-elect=true \
  --node-cidr-mask-size=24 \
  --root-ca-file=/opt/kubernetes/pki/ca/cert/ca.pem \
  --service-account-private-key-file=/opt/kubernetes/pki/ca/cert/ca-key.pem \
  --service-cluster-ip-range=10.68.0.0/16 \
  --use-service-account-credentials=true \
  --v=2
```

```bash
# ssh k8s-master-03
$ ssh kube@192.168.1.206

$ /opt/kubernetes/bin/kube-controller-manager \
  --allocate-node-cidrs=true \
  --authentication-kubeconfig=/opt/kubernetes/kubeconfig/kube-controller-manager.kubeconfig \
  --authorization-kubeconfig=/opt/kubernetes/kubeconfig/kube-controller-manager.kubeconfig \
  --bind-address=192.168.1.206 \
  --cluster-cidr=172.20.0.0/16 \
  --cluster-name=kubernetes \
  --cluster-signing-cert-file=/opt/kubernetes/pki/ca/cert/ca.pem \
  --cluster-signing-key-file=/opt/kubernetes/pki/ca/cert/ca-key.pem \
  --kubeconfig=/opt/kubernetes/kubeconfig/kube-controller-manager.kubeconfig \
  --leader-elect=true \
  --node-cidr-mask-size=24 \
  --root-ca-file=/opt/kubernetes/pki/ca/cert/ca.pem \
  --service-account-private-key-file=/opt/kubernetes/pki/ca/cert/ca-key.pem \
  --service-cluster-ip-range=10.68.0.0/16 \
  --use-service-account-credentials=true \
  --v=2
```

`--cluster-cidr`：Pod地址范围

`--leader-elect=true`：开启kube-controller-manager leader选举。kube-controller-manager配置为高空用时，在同一时间内只能有一个工作，通过leader elect选举得到。

`--service-cluster-ip-range`：Service地址范围

&nbsp;

# 3. Smoke test

## 3.1. 查看kube-controller-manager@k8s-master-01日志

可以看到kube-controller-manager在启动中尝试成为leader，由于k8s-master-01上的kube-controller-manager是最先启动的，因此它很自然地就成为了leader。kube-controller-manager启动完毕后，deployment controller创建了一个nginx replicaset；nginx replicaset创建了对应的3个Pod（元数据层面）。

![](../pictures/kube_controller_manager_attemping_become_leader_1.jpg)

![](../pictures/kube_controller_manager_log_2.jpg)

## 3.2. 查看kube-controller-manager@k8s-master-02日志

可以看到k8s-master-02上的kube-controller-manager在启动中，尝试成为leader，但并没有成功。这是因为k8s-master-01上的kube-controller-manager已经是leader了。

![](../pictures/kube_controller_manager_attemping_become_leader_2.jpg)

## 3.3. 查看kube-controller-manager@k8s-master-03日志

可以看到k8s-master-03上的kube-controller-manager在启动中，也尝试成为leader，但也没有成功。同理，这是因为k8s-master-01上的kube-controller-manager已经是leader了。

![](../pictures/kube_controller_manager_attemping_become_leader_3.jpg)

## 3.4. 通过kubectl查看nginx deployment

现在可以查询到对应的Nginx pod了。由于目前还没有部署kube-scheduler，pod还无法被调度，因此显示的是Pending状态。

![](../pictures/kubectl_get_nginx_deployment_and_pod_2.jpg)

## 3.5. 查看etcd中nginx deployment相关的元数据

可以观察到deployment controller创建的相关元数据。

```bash
# ssh to operation-machine
$ ssh 192.168.1.200

$ etcdctl --cacert="/opt/kubernetes/pki/ca/cert/ca.pem" --cert="/opt/kubernetes/pki/etcd/cert/etcd.pem" --key="/opt/kubernetes/pki/etcd/cert/etcd-key.pem" --endpoints=192.168.1.201:2379 get / --prefix --keys-only | grep nginx
```

![](../pictures/etcd_get_nginx_metadata_3.jpg)

## 3.6. 停止kube-controller-manager@k8s-master-01

并观察`k8s-master-02`和`k8s-master-03`上的kube-controller-manager日志，发现此时`k8s-master-02`上的`kube-controller-manager`成为了`leader`。

![](../pictures/kube_controller_manager_attemping_become_leader_4.jpg)

## 3.7. 停止kube-controller-manager@k8s-master-02

并观察`k8s-master-03`上的kube-controller-manager日志，发现此时`k8s-master-03`上的`kube-controller-manager`成为了`leader`。

![](../pictures/kube_controller_manager_attemping_become_leader_5.jpg)

&nbsp;

根据日志来看，kube-controller-manager的leader选举应该是利用etcd的lease机制来实现的，但这也是目前个人根据观察日志得到的推测，具体还是得去通过查阅源码来验证。

&nbsp;

# 4. 配置system service（optional）

步骤2.6是以命令行的方式启动，以便在部署过程中观察日志并排障。当部署成功且`smoke test`通过后，可以用`system service`的方式启动，以便测试机器重启后可以自动拉起`kube-controller-manager`服务。

## 4.1. 配置kube-controller-manager@k8s-master-01系统服务

```bash
# ssh to k8s-master-01
$ ssh kube@192.168.1.204

$ sudo vim /etc/systemd/system/kube-controller-manager.service
```

```bash
[Unit]
Description=Kubernetes Controller Manager
Documentation=https://github.com/GoogleCloudPlatform/kubernetes

[Service]
ExecStart=/opt/kubernetes/bin/kube-controller-manager \
  --allocate-node-cidrs=true \
  --authentication-kubeconfig=/opt/kubernetes/kubeconfig/kube-controller-manager.kubeconfig \
  --authorization-kubeconfig=/opt/kubernetes/kubeconfig/kube-controller-manager.kubeconfig \
  --bind-address=192.168.1.204 \
  --cluster-cidr=172.20.0.0/16 \
  --cluster-name=kubernetes \
  --cluster-signing-cert-file=/opt/kubernetes/pki/ca/cert/ca.pem \
  --cluster-signing-key-file=/opt/kubernetes/pki/ca/cert/ca-key.pem \
  --kubeconfig=/opt/kubernetes/kubeconfig/kube-controller-manager.kubeconfig \
  --leader-elect=true \
  --node-cidr-mask-size=24 \
  --root-ca-file=/opt/kubernetes/pki/ca/cert/ca.pem \
  --service-account-private-key-file=/opt/kubernetes/pki/ca/cert/ca-key.pem \
  --service-cluster-ip-range=10.68.0.0/16 \
  --use-service-account-credentials=true \
  --v=2
Restart=always
RestartSec=5

[Install]
WantedBy=multi-user.target
```

```bash
$ sudo systemctl enable kube-controller-manager.service

$ sudo systemctl daemon-reload && sudo systemctl restart kube-controller-manager.service
```

## 4.2. 配置kube-controller-manager@k8s-master-02系统服务

```bash
# ssh to k8s-master-02
$ ssh kube@192.168.1.205

$ sudo vim /etc/systemd/system/kube-controller-manager.service
```

```bash
[Unit]
Description=Kubernetes Controller Manager
Documentation=https://github.com/GoogleCloudPlatform/kubernetes

[Service]
ExecStart=/opt/kubernetes/bin/kube-controller-manager \
  --allocate-node-cidrs=true \
  --authentication-kubeconfig=/opt/kubernetes/kubeconfig/kube-controller-manager.kubeconfig \
  --authorization-kubeconfig=/opt/kubernetes/kubeconfig/kube-controller-manager.kubeconfig \
  --bind-address=192.168.1.205 \
  --cluster-cidr=172.20.0.0/16 \
  --cluster-name=kubernetes \
  --cluster-signing-cert-file=/opt/kubernetes/pki/ca/cert/ca.pem \
  --cluster-signing-key-file=/opt/kubernetes/pki/ca/cert/ca-key.pem \
  --kubeconfig=/opt/kubernetes/kubeconfig/kube-controller-manager.kubeconfig \
  --leader-elect=true \
  --node-cidr-mask-size=24 \
  --root-ca-file=/opt/kubernetes/pki/ca/cert/ca.pem \
  --service-account-private-key-file=/opt/kubernetes/pki/ca/cert/ca-key.pem \
  --service-cluster-ip-range=10.68.0.0/16 \
  --use-service-account-credentials=true \
  --v=2
Restart=always
RestartSec=5

[Install]
WantedBy=multi-user.target
```

```bash
$ sudo systemctl enable kube-controller-manager.service

$ sudo systemctl daemon-reload && sudo systemctl restart kube-controller-manager.service
```

## 4.3. 配置kube-controller-manager@k8s-master-03系统服务

```bash
# ssh to k8s-master-03
$ ssh kube@192.168.1.206

$ sudo vim /etc/systemd/system/kube-controller-manager.service
```

```bash
[Unit]
Description=Kubernetes Controller Manager
Documentation=https://github.com/GoogleCloudPlatform/kubernetes

[Service]
ExecStart=/opt/kubernetes/bin/kube-controller-manager \
  --allocate-node-cidrs=true \
  --authentication-kubeconfig=/opt/kubernetes/kubeconfig/kube-controller-manager.kubeconfig \
  --authorization-kubeconfig=/opt/kubernetes/kubeconfig/kube-controller-manager.kubeconfig \
  --bind-address=192.168.1.206 \
  --cluster-cidr=172.20.0.0/16 \
  --cluster-name=kubernetes \
  --cluster-signing-cert-file=/opt/kubernetes/pki/ca/cert/ca.pem \
  --cluster-signing-key-file=/opt/kubernetes/pki/ca/cert/ca-key.pem \
  --kubeconfig=/opt/kubernetes/kubeconfig/kube-controller-manager.kubeconfig \
  --leader-elect=true \
  --node-cidr-mask-size=24 \
  --root-ca-file=/opt/kubernetes/pki/ca/cert/ca.pem \
  --service-account-private-key-file=/opt/kubernetes/pki/ca/cert/ca-key.pem \
  --service-cluster-ip-range=10.68.0.0/16 \
  --use-service-account-credentials=true \
  --v=2
Restart=always
RestartSec=5

[Install]
WantedBy=multi-user.target
```

```bash
$ sudo systemctl enable kube-controller-manager.service

$ sudo systemctl daemon-reload && sudo systemctl restart kube-controller-manager.service
```

&nbsp;

# 5. 参考

1. [Kubernetes - PKI certificates and requirements](https://kubernetes.io/docs/setup/best-practices/certificates/)
2. [Kubernetes - Generate Certificates Manually](https://kubernetes.io/docs/tasks/administer-cluster/certificates/)
3. [Kubernetes - 使用 kubeconfig 文件组织集群访问](https://kubernetes.io/zh-cn/docs/concepts/configuration/organize-cluster-access-kubeconfig/)
4. [kubeasz - 生成 kubeconfig 配置文件](https://github.com/easzlab/kubeasz/blob/master/docs/setup/01-CA_and_prerequisite.md#%E7%94%9F%E6%88%90-kubeconfig-%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6)
5. [kubeasz Create kube-controller-manager-kubeconfig.yml](https://github.com/easzlab/kubeasz/blob/master/roles/deploy/tasks/create-kube-controller-manager-kubeconfig.yml)
6. [kubeasz - kube-controller-manager.service.j2](https://github.com/easzlab/kubeasz/blob/master/roles/kube-master/templates/kube-controller-manager.service.j2)
