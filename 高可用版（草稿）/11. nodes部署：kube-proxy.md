必须在部署calico网络插件之间先部署kube-proxy，因为部署calico时，calico需要用到kube service来访问apiserver。若kube-proxy没有部署完成，在部署Calico时会遇到问题。



# 1. 前置步骤

- 完成《01.前置准备》
- 完成《02. 私有CA准备》
- 完成《03. 为kubectl配置admin user》
- 完成《04. control plane部署：etcd集群部署》
- 完成《05. control plane部署：kube-apiserver》
- 完成《06. control plane部署：kube-controller-manager》
- 完成《07. control plane部署：kube-scheduler》
- 完成《08. nodes部署：containerd》
- 完成《09. nodes部署：kubelet》



# 2. 安装kube-proxy

## 2.1. 创建kube-proxy证书签名请求

由于kubelet证书在签发时，对CN字段有特殊格式要求，以用于在接入kube-apiserver时，进行身份验证及授予预设的角色权限，因此此处的CN字段值为system:kube-proxy。

```bash
# ssh operation-machine
$ ssh kube@192.168.1.200

$ mkdir -p /opt/kubernetes/pki/kube-proxy/cert

$ vim /opt/kubernetes/pki/kube-proxy/kube-proxy-csr.json
```

```bash
{
  "CN": "system:kube-proxy",
  "hosts": [],
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "Shanghai",
      "L": "Shanghai",
      "O": "k8s",
      "OU": "System"
    }
  ]
}
```

## 2.2. 创建kube-proxy证书和私钥

```bash
# ssh operation-machine
$ ssh kube@192.168.1.200

$ cfssl gencert \
-ca=/opt/kubernetes/pki/ca/cert/ca.pem \
-ca-key=/opt/kubernetes/pki/ca/cert/ca-key.pem \
-config=/opt/kubernetes/pki/ca/ca-config.json \
-profile=kubernetes /opt/kubernetes/pki/kube-proxy/kube-proxy-csr.json | cfssljson -bare /opt/kubernetes/pki/kube-proxy/cert/kube-proxy

$ ls /opt/kubernetes/pki/kube-proxy/cert/
... kube-proxy-key.pem  kube-proxy.pem ...
```

## 2.3. 生成kube-proxy.kubeconfig

```bash
# ssh operation-machine
$ ssh kube@192.168.1.200

$ kubectl config set-cluster kubernetes --certificate-authority=/opt/kubernetes/pki/ca/cert/ca.pem --embed-certs=true --server=https://192.168.1.204:6443 --kubeconfig=/opt/kubernetes/kubeconfig/kube-proxy.kubeconfig

$ kubectl config set-credentials kube-proxy --client-certificate=/opt/kubernetes/pki/kube-proxy/cert/kube-proxy.pem --embed-certs=true --client-key=/opt/kubernetes/pki/kube-proxy/cert/kube-proxy-key.pem --kubeconfig=/opt/kubernetes/kubeconfig/kube-proxy.kubeconfig

$ kubectl config set-context default --cluster=kubernetes --user=kube-proxy --kubeconfig=/opt/kubernetes/kubeconfig/kube-proxy.kubeconfig

$ kubectl config use-context default --kubeconfig=/opt/kubernetes/kubeconfig/kube-proxy.kubeconfig
```

## 2.4.分发kube-proxy.kubeconfig

分发至各work node。

```bash
# ssh operation-machine
$ ssh kube@192.168.1.200

$ scp /opt/kubernetes/kubeconfig/kube-proxy.kubeconfig kube@192.168.1.207:/opt/kubernetes/kubeconfig/kube-proxy.kubeconfig

$ scp /opt/kubernetes/kubeconfig/kube-proxy.kubeconfig kube@192.168.1.208:/opt/kubernetes/kubeconfig/kube-proxy.kubeconfig

$ scp /opt/kubernetes/kubeconfig/kube-proxy.kubeconfig kube@192.168.1.209:/opt/kubernetes/kubeconfig/kube-proxy.kubeconfig
```

## 2.5. 创建kube-proxy的配置文件

```bash
# ssh k8s-node-01
$ ssh 192.168.1.207

$ vim /var/lib/kube-proxy/kube-proxy-config.yaml
```

```bash
kind: KubeProxyConfiguration
apiVersion: kubeproxy.config.k8s.io/v1alpha1
bindAddress: 0.0.0.0
clientConnection:
  kubeconfig: "/opt/kubernetes/kubeconfig/kube-proxy.kubeconfig"
# clusterCIDR即Pod IP地址的范围。根据clusterCIDR判断集群内部和外部流量，配置clusterCIDR选项后，kube-proxy 会对访问Service IP的请求做SNAT
clusterCIDR: "172.20.0.0/16" 
conntrack:
  maxPerCore: 32768
  min: 131072
  tcpCloseWaitTimeout: 1h0m0s
  tcpEstablishedTimeout: 24h0m0s
healthzBindAddress: 0.0.0.0:10256
# hostnameOverride值必须与kubelet启动参数中的值对应一致，否则 kube-proxy启动后会找不到该Node，从而不会创建任何iptables规则
hostnameOverride: "192.168.1.207"
metricsBindAddress: 0.0.0.0:10249
mode: "ipvs"
```

## 2.6. 启动kube-proxy

```bash
$ sudo kube-proxy --config=/var/lib/kube-proxy/kube-proxy-config.yaml
```



# 3. Smoke test

## 3.1. 从集群内部访问Nginx service IP

暴露nginx service，并创建busybox，从busybox内访问nginx service的ip地址

```bash
# ssh operation-machine
$ ssh kube@192.168.1.200

kubectl expose nginx

kubectl exec -it nginx-xxx-xxxx -- /bin/bash

curl nginx
```

## 3.2. 从NodePort访问Nginx service 

```bash
ssh operation-machine

kubectl expose nginx --type nodeport

curl 192.168.1.207:nodeport
```

无论使用上述两种方式都会有如下现象：有时nginx能返回欢迎界面，有时却访问不通。出现这个现象的原因是，kube service本质是对pod的负载均衡，在本文档中，它通过系统的ipvs转发规则实现。当向192.168.1.207发起请求时，若被ipvs规则负载到运行在192.168.1.207节点上的pod时，此时请求能够通；若负载到其他节点上的Pod时，此时由于还没有部署集群CNI插件，pod还无法跨界点通信，因此请求就不通了。

## 3.3. 从集群内部访问Nginx service 域名

直接不通，因为还没有部署DNS插件。



# 4. 配置system service（optional）

步骤2.6是以命令行的方式启动，以便在部署过程中观察日志并排障。当部署成功且`smoke test`通过后，可以用`system service`的方式启动，以便测试机器重启后可以自动拉起`kube-proxy`服务。

```bash
# ssh k8s-node-01
$ ssh kube@192.168.1.207

$ sudo vim /etc/systemd/system/kube-proxy.service
```

```bash
[Unit]
Description=Kubernetes Kube-Proxy Server
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=network.target

[Service]
WorkingDirectory=/var/lib/kube-proxy
ExecStart=/opt/kubernetes/bin/kube-proxy \
  --config=/var/lib/kube-proxy/kube-proxy-config.yaml
Restart=always
RestartSec=5
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
```

```bash
$ sudo systemctl enable kube-proxy.service

$ sudo systemctl daemon-reload && sudo systemctl restart kube-proxy.service
```



# 5. 参考

1. [kubeasz - install_kube_node.md](https://github.com/easzlab/kubeasz/blob/master/docs/setup/05-install_kube_node.md)
2. [kubeasz - kube-node Ansible roles](https://github.com/easzlab/kubeasz/tree/master/roles/kube-node)
3. [kubeasz - tasks Ansible role, sign cert](https://github.com/easzlab/kubeasz/blob/master/roles/deploy/tasks/main.yml)
4. [kubeasz - create-kube-proxy-kubeconfig.yml](https://github.com/easzlab/kubeasz/blob/master/roles/deploy/tasks/create-kube-proxy-kubeconfig.yml)
5. [kubeasz - deploy Ansible roles](https://github.com/easzlab/kubeasz/tree/master/roles/deploy)
6. [kubeasz - Create kube-proxy-kubeconfig.yml](https://github.com/easzlab/kubeasz/blob/master/roles/deploy/tasks/create-kube-proxy-kubeconfig.yml)
